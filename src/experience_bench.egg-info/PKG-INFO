Metadata-Version: 2.4
Name: experience-bench
Version: 0.1.0
Summary: LLM benchmarking tool for measuring effect of system-prompt experience years
Author: experience-bench
License: MIT
Requires-Python: >=3.10
Description-Content-Type: text/markdown
Requires-Dist: httpx>=0.27.0
Requires-Dist: PyYAML>=6.0.2
Requires-Dist: rich>=13.7.1
Requires-Dist: plotly>=5.24.1
Requires-Dist: tqdm>=4.66.0
Provides-Extra: dev
Requires-Dist: pytest>=8.0.0; extra == "dev"

# experience-bench

Benchmarks LLMs one-shot on a config-driven problem while varying a system prompt like:

> "You are a software engineer with **X years** of experience."

Measures:
- Correctness (Part A + Part B)
- Speed: non-streaming TTLT (time until full response is received)
- Code performance: execution runtime

Providers supported:
- OpenRouter (remote)
- Ollama (local)
- Azure OpenAI (Responses API via Azure deployment)

## Quickstart

```bash
python -m venv .venv
source .venv/bin/activate
pip install -e .

cp .env.example .env
# edit .env with your keys / endpoints

# Edit these files locally (do not commit AoC text if you don't want to):
# - benchmarks/aoc_2025_day01_statement.txt
# - benchmarks/aoc_2025_day01_input.txt
# - benchmarks/aoc_2025_day01.yaml (expected answers)

experience-bench run --benchmark benchmarks/aoc_2025_day01.yaml --out results.jsonl
experience-bench report --in results.jsonl --out reports/report.html
```

Open the report:

```bash
open reports/report.html
```

## Notes
- This tool executes generated code. It runs in a temp directory with a timeout, but it is not a hardened sandbox.
- Token counts are reported **per model_key** only (do not compare token totals across different models).
